---
title: "Take-home Exercise 2: Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows"
author: "Chai Zhixuan"
editor: visual
date: 2023-12-06
date-format: long
date-modified: "last-modified"
---

# **1 Overview**

In this study, I will continue my geospatial analytics learning journey to demonstrate the potential value of geospatial data science and analysis (GDSA) to integrate publicly available data from multiple sources for building a spatial interaction models to determine factors affecting urban mobility patterns of public bus transit.

# **2 Getting Started**

## **2.1 Setting the Analytical Tools**

The code chunk below installs and loads the various packages

```{r}
pacman::p_load(tmap, sf, DT, stplanr, sp,
               performance,reshape2,httr,
               ggpubr, tidyverse)
```

# **3 Data Preparation**

## **3.1 Data**

### **3.1.1 Importing the OD data**

Firstly, we will import the *Passenger Volume by Origin Destination Bus Stops* data set downloaded from LTA DataMall by using `read_csv()` of **readr** package. Oct 2023 data will be used. Detailed explanation of how the dataset is obtained is in Take Home Exercise 1.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

Let use display the *odbus* tibble data table by using the code chunk below.

```{r}
glimpse(odbus)
```

A quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.

```{r}
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
```

For the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o'clock. This will mainly be people going to school or work.

```{r}
odbus6_9 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE,
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

We will save the output in rds format for future used.

```{r}
write_rds(odbus6_9, "data/rds/odbus6_9.rds")
```

The code chunk below will be used to import the save odbus6_9.rds into R environment.

```{r}
odbus6_9 <- read_rds("data/rds/odbus6_9.rds")
```

### **3.1.2 Importing Geospatial data into R**

For the purpose of this exercise, three geospatial data will be used. They are:

-   BusStop: This data provides the location of bus stop as at last quarter of 2023.

-   MPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.

-   Hexagon: analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

First 2 data sets are in ESRI shapefile format. Last file will be derived from R in the later setion

```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
```

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

```{r}
mpsz
```

```{r}
mpsz <- write_rds(mpsz, "data/rds/mpsz.rds")
```

# **4 Geospatial data wrangling**

## **4.1 Combining Busstop and mpsz**

Code chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.

```{r}
busstop_mpsz <- st_intersection(busstop, mpsz) %>%
  select(BUS_STOP_N, SUBZONE_C) 
```

```{r}
write_rds(busstop_mpsz, "data/rds/busstop_mpsz.rds")  
```

## 4.2 Creating Hexagon layer

Now, I am going to create a hexagon layer:

```{r}
# cell size of layer of 250m
area_honeycomb_grid = st_make_grid(busstop_mpsz, c(750, 750), what = "polygons", square = FALSE, crs = 3414)

# To sf and add grid ID
honeycomb_grid_sf = st_sf(area_honeycomb_grid)
```

```{r}
st_write(honeycomb_grid_sf, "data/geospatial/hexagon.shp",append=TRUE)
```

```{r}
hexagon <- st_read(dsn = "data/geospatial",
                   layer = "hexagon") %>%
  st_transform(crs = 3414)
```

## 4.3 Combine Hexagon and Busstop_Mpsz

Next, we are going to combine the datset busstop_mpsz and hexagon

```{r}
od_data <- st_join(busstop_mpsz , hexagon,
            by = c("geometry" = "geometry")) 
```

## 4.4 Combine Hexagon and Busstop

Combing Hexagon and busstop will give the hexagon shape to the

```{r}
hexagon_busstop <- st_join(hexagon, busstop, by = c("FID" = "FID"))
```

```{r}
hexagon_busstop <- hexagon_busstop %>%
  drop_na() %>%
  group_by(FID)
```

```{r}
write_rds(hexagon_busstop, "data/rds/hexagon_busstop.rds")
```

## 4.5 Combine odbus6_9 with od_data

```{r}
od_data_1 <- left_join(odbus6_9 , od_data,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_SZ = SUBZONE_C,
         DESTIN_BS = DESTINATION_PT_CODE)
```

Before continue, it is a good practice for us to check for duplicating records.

```{r}
duplicate <- od_data_1 %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

If duplicated records are found, the code chunk below will be used to retain the unique records.

```{r}
od_data_1 <- unique(od_data_1)
```

It will be a good practice to confirm if the duplicating records issue has been addressed fully.

Next, we will update od_data data frame with Combine Hexagon and Busstop_Mpsz.

```{r}
od_data_2 <- left_join(od_data_1 , od_data,
            by = c("DESTIN_BS" = "BUS_STOP_N")) 
```

Before continue, it is a good practice for us to check for duplicating records.

```{r}
duplicate <- od_data_2 %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

If duplicated records are found, the code chunk below will be used to retain the unique records.

```{r}
od_data_2 <- unique(od_data_2)
```

```{r}
od_data_2 <- od_data_2 %>%
  drop_na() %>%
  group_by(FID.x, FID.y) %>%
  summarise(MORNING_PEAK = sum(TRIPS))
```

It is time to save the output into an rds file format.

```{r}
write_rds(od_data_2, "data/rds/od_data_2.rds")
```

```{r}
od_data_2 <- read_rds("data/rds/od_data_2.rds")
```

# **5 Visualising Spatial Interaction**

## **5.1 Removing intra-zonal flows**

I will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows at the hexagon level. O-D matrix is as shown:

```{r}
od_data_3 <- od_data_2[od_data_2$FID.x!=od_data_2$FID.y,]
```

```{r}
head(od_data_3)
```

## 5.2 **Creating desire lines**

In this code chunk below, `od2line()` of **stplanr** package is used to create the desire lines at the hexagon layer.

```{r}
flowLine <- od2line(flow = od_data_3, 
                    zones = hexagon,
                    zone_code = "FID")
```

## 5.3 **Visualising the desire lines**

To visualise the resulting desire lines, the code chunk below is used.

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
flowLine %>%  
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)
```

## 5.4 Morning Peak (Greater than or equal to 5000)

When the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
flowLine %>%  
  filter(MORNING_PEAK >= 5000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)
```

For passengers volume greater or equal to 5000, we can observe from the map above that between Woodlands and area such as Tuas, Jurong, Woodlands and Tampines areas. There are also small pockets of areas in the central of Singapore.

## 5.5 Morning Peak (Greater than or equal to 10000)

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
flowLine %>%  
  filter(MORNING_PEAK >= 10000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)
```

For passenger volumes greater than 10,000, we observe in areas between Woodlands and Tuas, and also Woodlands and Tampines.

## 5.6 Morning Peak (Greater than or equal to 50000)

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
flowLine %>%  
  filter(MORNING_PEAK >= 50000) %>%
tm_shape() +
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)
```

For passengers volume equal or greater than 50,000, it is concentrated in Woodlands region.

# 6 Assemble propulsive and attractiveness variables

## 6.1 Geospatial

For geospatial, I will be using the following:

1.  *Business:* This dataset is used to illustrated people commuting to work

```{r}
business <- st_read(dsn = "data/geospatial",
                   layer = "Business") %>%
  st_transform(crs = 3414)
```

```{r}
hexagon_busstop$`BUSINESS_COUNT`<- lengths(
  st_intersects(
    hexagon_busstop, business))
```

```{r}
summary(hexagon_busstop$BUSINESS_COUNT)
```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
tm_shape(business) +
  tm_dots() 
```

2.  *FinServ:* This dataset is used to illustrate people working in the financial services sector

```{r}
finserv <- st_read(dsn = "data/geospatial",
                   layer = "FinServ") %>%
  st_transform(crs = 3414)
```

```{r}
hexagon_busstop$`FINANCE_COUNT`<- lengths(
  st_intersects(
    hexagon_busstop, finserv))
```

```{r}
summary(hexagon_busstop$FINANCE_COUNT)
```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
tm_shape(finserv) +
  tm_dots() 
```

3.  Train Station Exit Layer: This dataset is used to indicate if there is any potential transition from train station to busstop

```{r}
train <- st_read(dsn = "data/geospatial",
                   layer = "Train_Station_Exit_Layer") %>%
  st_transform(crs = 3414)
```

```{r}
hexagon_busstop$`TRAIN_COUNT`<- lengths(
  st_intersects(
    hexagon_busstop, train))
```

```{r}
summary(hexagon_busstop$TRAIN_COUNT)
```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
tm_shape(train) +
  tm_dots() 
```

4.  Rapid Transit System Station

```{r}
mrt <- st_read(dsn = "data/geospatial",
                   layer = "RapidTransitSystemStation") %>%
  st_transform(crs = 3414)
```

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
tm_shape(train) +
  tm_dots() 
```

::: callout-note
tmap_options(check.and.fix = TRUE) is used as Non closed ring is detected in this dataset
:::

::: callout-important
Due to issue with the shape file, it will not be used in **Spatial Interaction Models**
:::

## 6.2 Aspatial

For aspatial, I will be using the following:

1.  HDB: This dataset is used to understand the location of HBD in the hexagon layer

```{r}
data <- read.csv("data/aspatial/hdb.csv")
```

```{r}
coordinates <- data[, c("lng", "lat")]  
spatial_points <- SpatialPointsDataFrame(coordinates, data)
```

Convert to CRS3414:

```{r}

# Create a SpatialPoints object
coordinates <- data[, c("lng", "lat")]
spatial_points <- SpatialPoints(coords = coordinates)

# Define the current CRS (WGS84 - EPSG:4326)
proj4string(spatial_points) <- CRS("+proj=longlat +datum=WGS84")

# Convert SpatialPoints to an sf object
sf_points <- st_as_sf(spatial_points)

# Define EPSG:3414 CRS
epsg_3414_crs <- st_crs(3414)

# Transform the sf object to EPSG:3414
sf_points_3414 <- st_transform(sf_points, crs = epsg_3414_crs)

# Convert back to SpatialPoints
spatial_points_3414 <- as(sf_points_3414, "Spatial")


```

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
tm_shape(spatial_points_3414) +
  tm_dots()
```

```{r}
sf_spatial_points_3414 <- st_as_sf(spatial_points_3414)

intersections <- st_intersects(hexagon_busstop, sf_spatial_points_3414)

hexagon_busstop$HDB_COUNT <- lengths(intersections)
```

```{r}
summary(hexagon_busstop$HDB_COUNT)
```

2.  *School Directory and Information* - Dataset is taken from [Data\@Gov](https://beta.data.gov.sg/). This information is useful as the morning period will consist of students going to school

Concepts learned from In-Class Ex 4 will be used to extract the data from SLA API

```{r}
url<-"https://www.onemap.gov.sg/api/common/elastic/search"

csv<-read_csv("data/aspatial/Generalinformationofschools.csv")
postcodes<-csv$`postal_code`

found<-data.frame()
not_found<-data.frame()

for(postcode in postcodes){
  query<-list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')
  res<- GET(url,query=query)
  
  if((content(res)$found)!=0){
    found<-rbind(found,data.frame(content(res))[4:13])
  } else{
    not_found = data.frame(postcode)
  }
}
```

Next, the code chunk below will be used to combine both *found* and *not_found* data.frames into a single tibble data.frame called *merged*. At the same time, we will write *merged* and *not_found* tibble data.frames into two separate csv files called *schools* and *not_found* respectively.

```{r}
merged = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)
write.csv(merged, file = "data/aspatial/schools.csv")
write.csv(not_found, file = "data/aspatial/not_found.csv")
```

I will manually clean up for ZhengHua Secondary School ([link](https://www.google.com/search?q=zhenghua+secondary+school&rlz=1C1YTUH_enSG1024SG1038&oq=zhe&gs_lcrp=EgZjaHJvbWUqBggAEEUYOzIGCAAQRRg7Mg0IARAuGK8BGMcBGIAEMgYIAhBFGEAyBggDEEUYOTIKCAQQLhixAxiABDINCAUQLhivARjHARiABDINCAYQLhiDARixAxiABDIGCAcQRRg80gEIMzMyM2owajeoAgCwAgA&sourceid=chrome&ie=UTF-8)). File is saved as schools1.csv

Next, I will import *schools1.csv* into R environment and at the same time tidying the data by selecting only the necessary fields as well as rename some fields.

```{r}
schools <- read_csv("data/aspatial/schools1.csv") %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)
```

I convert schools1 tibble data.frame data into a simple feature tibble data.frame called *schools_sf* by using values in latitude and longitude fields.

```{r}
schools_sf <- st_as_sf(schools, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```

Plot is shown below:

```{r}
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hexagon_busstop) +
  tm_polygons() +
tm_shape(schools_sf) +
  tm_dots()
```

I will count count the number of schools located inside the hexagon layer.

```{r}
hexagon_busstop$`SCHOOL_COUNT`<- lengths(
  st_intersects(
    hexagon_busstop, schools_sf))
```

I will examine the summary statistics of the derived variable.

```{r}
summary(hexagon_busstop$`SCHOOL_COUNT`)
```

::: callout-important
The summary statistics above reveals that there are excessive 0 values in SCHOOL_COUNT field. If `log()` is going to use to transform this field, additional step is required to ensure that all 0 will be replaced with a value between 0 and 1 but not 0 neither 1.
:::

The data will be joined with od_data:

```{r}
hexagon_busstop_tidy <- hexagon_busstop %>%
  st_drop_geometry() %>%
  select(FID, SCHOOL_COUNT, BUSINESS_COUNT, FINANCE_COUNT,TRAIN_COUNT,HDB_COUNT)
```

```{r}
flow_data <- od_data_2 %>%
  left_join(hexagon_busstop_tidy,
            by = c("FID.y" = "FID"))
```

::: callout-note
FID.y is the destination. It is unique join field between od_data_2 and hexagon_busstop_tidy.
:::

```{r}
summary(flow_data)
```

The code chunk below will be used to replace zero values to 0.99.

```{r}
flow_data$SCHOOL_COUNT <- ifelse(
  flow_data$SCHOOL_COUNT == 0,
  0.99, flow_data$SCHOOL_COUNT)
flow_data$BUSINESS_COUNT <- ifelse(
  flow_data$BUSINESS_COUNT == 0,
  0.99, flow_data$BUSINESS_COUNT)
flow_data$FINANCE_COUNT <- ifelse(
  flow_data$FINANCE_COUNT == 0,
  0.99, flow_data$FINANCE_COUNT)
flow_data$TRAIN_COUNT <- ifelse(
  flow_data$TRAIN_COUNT == 0,
  0.99, flow_data$TRAIN_COUNT)
flow_data$HDB_COUNT <- ifelse(
  flow_data$HDB_COUNT == 0,
  0.99, flow_data$HDB_COUNT)
```

```{r}
summary(flow_data)
```

Next we will remove duplicate record:

```{r}
duplicate <- flow_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
flow_data <- unique(flow_data)
```

```{r}
summary(flow_data)
```

I will save flow_data sf tibble data.frame into an rds file and call the file *flow_data_tidy*.

```{r}
write_rds(flow_data,
          "data/rds/flow_data_tidy.rds")
```

The explanatory variables to be used in the **Spatial Interaction Modelling** will be :

1.  Business
2.  Finance Service
3.  Train Station Exit Layer
4.  HDB
5.  School Directory and information

# 7 **Computing Distance Matrix**

## 7.1 **Converting from sf data.table to SpatialPolygonsDataFrame**

First [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) will be used to convert *mpsz* from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below

```{r}
hexagon_busstop_sp <- as(hexagon_busstop, "Spatial")
hexagon_busstop_sp
```

## 7.2 **Computing the distance matrix**

Next, [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) of sp package will be used to compute the Euclidean distance between the centroids of the hexagon layer.

```{r}
dist <- spDists(hexagon_busstop_sp, 
                longlat = FALSE)
head(dist, n=c(10, 10))
```

Next, I will rename column and rows based on FID

```{r}
sz_names <- hexagon_busstop$FID
```

```{r}
colnames(dist) <- paste0(sz_names)
rownames(dist) <- paste0(sz_names)
```

Next, we will pivot the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.

```{r}
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

To update the intra-zonal distances, I will select and find out the minimum value of the distance by using `summary()`.

```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```

Next, a constant distance value of 300m is added into intra-zones distance.

```{r}
distPair$dist <- ifelse(distPair$dist == 0,
                        300, distPair$dist)
```

The code chunk below will be used to check the result data.frame.

```{r}
distPair %>%
  summary()
```

The code chunk below is used to rename the origin and destination fields.

```{r}
distPair <- distPair %>%
  rename(orig = Var1,
         dest = Var2)
```

Lastly, the code chunk below is used to save the dataframe for future use.

```{r}
write_rds(distPair, "data/rds/distPair.rds") 
```

# 8 Spatial Interaction Modelling

## 8.1 Preparing Flow Data

```{r}
head(flow_data, 10)
```

### **8.1.1 Separating intra-flow from passenger volume df**

Code chunk below is used to add three new fields in `flow_data` dataframe.

```{r}
flow_data$FlowNoIntra <- ifelse(
  flow_data$FID.x == flow_data$FID.y, 
  0, flow_data$MORNING_PEAK)
flow_data$offset <- ifelse(
  flow_data$FID.x == flow_data$FID.y, 
  0.000001, 1)
```

### 8.1.2 **Combining flow data with distance value**

```{r}
flow_data$FID.x <- as.factor(flow_data$FID.x)
flow_data$FID.y <- as.factor(flow_data$FID.y)
```

Now, `left_join()` of **dplyr** will be used to *flow_data* dataframe and *distPair* dataframe. The output is called *flow_data1*.

```{r}
flow_data$FID.x <- as.integer(as.character(flow_data$FID.x))
flow_data$FID.y <- as.integer(as.character(flow_data$FID.y))


flow_data1 <- flow_data %>%
  left_join (distPair,
             by = c("FID.x" = "orig",
                    "FID.y" = "dest"))
```

We will remove duplicate

```{r}
duplicate <- flow_data1 %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
flow_data1 <- unique(flow_data1)
```

We will called the output data file *SIM_data*. it is in rds data file format.

```{r}
write_rds(flow_data1, "data/rds/SIM_data.rds")
```

## **8.2 Calibrating Spatial Interaction Models**

### 8.2.1 **Importing the modelling data**

```{r}
SIM_data <- read_rds("data/rds/SIM_data.rds")
```

### 8.2.2 **Visualising the dependent variable**

```{r}
ggplot(data = SIM_data,
       aes(x = MORNING_PEAK)) +
  geom_histogram()
```

Notice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.

Next, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.

```{r}
ggplot(data = SIM_data,
       aes(x = dist,
           y = MORNING_PEAK)) +
  geom_point() +
  geom_smooth(method = lm)
```

.

### **8.2.3 Checking for variables with zero values**

Since Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.

In the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in *SIM_data* data frame.

```{r}
summary(SIM_data)
```

Since we have replaced the zero values. No further action is required.

Change FID.x and FID.y to character

```{r}
SIM_data$FID.x <- as.character(SIM_data$FID.x)
SIM_data$FID.y <- as.character(SIM_data$FID.y)
```

Inter-zonal flow will be selected from flow_data and save into a new output data.frame called *inter_zonal_flow* by using the code chunk below.

```{r}
inter_zonal_flow <- SIM_data %>%
  filter(FlowNoIntra > 0)
```

```{r}
summary(inter_zonal_flow)
```

Remove NA:

```{r}
inter_zonal_flow <- na.omit(inter_zonal_flow)
```

```{r}
summary(inter_zonal_flow)
```

## 8.3 **Origin (Production) constrained SIM**

The code chunk used to calibrate to model is shown below. FID.x is the origin:

```{r}
orcSIM <- glm(formula = MORNING_PEAK ~ 
                  FID.x +
                  log(SCHOOL_COUNT)+
                  log(BUSINESS_COUNT)+
                  log(FINANCE_COUNT)+
                  log(HDB_COUNT)+
                  log(TRAIN_COUNT)+
                  log(dist) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(orcSIM)
```

### 8.3.1 **Goodness of fit**

```{r}
CalcRSquared <- function(observed, estimated){
  r <- cor(observed, estimated)
  R2 <- r^2
  R2
}
```

```{r}
CalcRSquared(orcSIM$data$MORNING_PEAK, orcSIM$fitted.values)
```

## 8.4 **Doubly constrained model**

```{r}
dbcSIM_Poisson <- glm(formula = MORNING_PEAK ~ 
                  FID.x +
                  FID.y +
                  log(SCHOOL_COUNT)+
                  log(BUSINESS_COUNT)+
                  log(FINANCE_COUNT)+
                  log(HDB_COUNT)+
                  log(TRAIN_COUNT)+
                  log(dist),
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(dbcSIM_Poisson)
```

```{r}
CalcRSquared(dbcSIM_Poisson$data$MORNING_PEAK,
             dbcSIM_Poisson$fitted.values)
```

# 8.5 **Model comparison**

## 8.5.1 **Statistical measures**

First of all, let us create a list called *model_list* by using the code chunk below.

```{r}
model_list <- list(
  Origin_Constrained = orcSIM,
  Doubly_Constrained = dbcSIM_Poisson)
```

Next, we will compute the RMSE of all the models in *model_list* file by using the code chunk below.

```{r}
compare_performance(model_list,
                    metrics = "RMSE")
```

The print above reveals that doubly constrained SIM is the best model among the two SIMs because it has the smallest RMSE value of 970.640.

## 8.5.2 **Visualising fitted values**

Firstly we will extract the fitted values from Origin-constrained Model by using the code chunk below.

```{r}
df <- as.data.frame(orcSIM$fitted.values) %>%
  round(digits = 0)
```

Next, we will append the fitted values into *inter_zonal_flow* data frame by using the code chunk below.

```{r}
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df) %>%
  rename(orcTRIPS = "orcSIM$fitted.values")
```

I will do the same for dbcSIM_Poisson

```{r}
df1 <- as.data.frame(dbcSIM_Poisson$fitted.values) %>%
  round(digits = 0)
```

```{r}
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df1) %>%
  rename(dbcTRIPS = "dbcSIM_Poisson$fitted.values")
```

Next, two scatterplots will be created by using [`geom_point()`](https://ggplot2.tidyverse.org/reference/geom_point.html) and other appropriate functions of [**ggplot2**](https://ggplot2.tidyverse.org/) package.

```{r}
orc_p <- ggplot(data = inter_zonal_flow,
                aes(x = orcTRIPS,
                    y = MORNING_PEAK)) +
  geom_point() +
  geom_smooth(method = lm) +
  coord_cartesian(xlim=c(0,100000),
                  ylim=c(0,100000))

dbc_p <- ggplot(data = inter_zonal_flow,
                aes(x = dbcTRIPS,
                    y = MORNING_PEAK)) +
  geom_point() +
  geom_smooth(method = lm) +
  coord_cartesian(xlim=c(0,100000),
                  ylim=c(0,100000))
```

Now, we will put all the graphs into a single visual for better comparison by using the code chunk below.

```{r}
ggarrange(orc_p, dbc_p,
          ncol = 2,
          nrow = 1)
```

# 9 Modelling Results
